From 07067aa3e4496072c0bca3e5c4cd9eda850b530e Mon Sep 17 00:00:00 2001
From: Alistair Coles <alistair.coles@hpe.com>
Date: Mon, 17 Oct 2016 20:38:52 +0100
Subject: [PATCH] Make ECDiskFileReader check fragment metadata
Origin: https://github.com/openstack/swift/commit/07067aa3e4496072c0bca3e5c4cd9eda850b530e

This patch makes the ECDiskFileReader check the validity of EC
fragment metadata as it reads chunks from disk and quarantine a
diskfile with bad metadata. This in turn means that both the object
auditor and a proxy GET request will cause bad EC fragments to be
quarantined.

This change is motivated by bug 1631144 which may result in corrupt EC
fragments being written to disk but appear valid to the object auditor
md5 hash and content-length checks.

NotImplemented:

 * perform metadata check when a read starts on any frag_size
   boundary, not just at zero

Related-Bug: #1631144
Closes-Bug: #1633647

This is a backport of commit 2a75091c58948fb664016c0e91e72acd313e4610

Change-Id: Ifa6a7f8aaca94c7d39f4aeb9d4fa3f59c4f6ee13
Co-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>
Co-Authored-By: Kota Tsuyuzaki <tsuyuzaki.kota@lab.ntt.co.jp>
---
 swift/obj/diskfile.py                   |  92 +++++++++-
 test/unit/__init__.py                   |  29 +++
 test/unit/obj/common.py                 |  50 ++++--
 test/unit/obj/test_auditor.py           |  81 ++++++++-
 test/unit/obj/test_diskfile.py          | 307 ++++++++++++++++++++++++++------
 test/unit/obj/test_server.py            | 122 +++++++++++--
 test/unit/obj/test_ssync.py             | 170 ++++++++++--------
 test/unit/proxy/controllers/test_obj.py |  20 +--
 8 files changed, 688 insertions(+), 183 deletions(-)

diff --git a/swift/obj/diskfile.py b/swift/obj/diskfile.py
index 81ae00a..fcb1a85 100644
--- a/swift/obj/diskfile.py
+++ b/swift/obj/diskfile.py
@@ -50,6 +50,8 @@
 from eventlet import Timeout
 from eventlet.hubs import trampoline
 import six
+from pyeclib.ec_iface import ECDriverError, ECInvalidFragmentMetadata, \
+    ECBadFragmentChecksum, ECInvalidParameter
 
 from swift import gettext_ as _
 from swift.common.constraints import check_mount, check_dir
@@ -1593,6 +1595,15 @@ def __init__(self, fp, data_file, obj_size, etag,
     def manager(self):
         return self._diskfile.manager
 
+    def _init_checks(self):
+        if self._fp.tell() == 0:
+            self._started_at_0 = True
+            self._iter_etag = hashlib.md5()
+
+    def _update_checks(self, chunk):
+        if self._iter_etag:
+            self._iter_etag.update(chunk)
+
     def __iter__(self):
         """Returns an iterator over the data file."""
         try:
@@ -1600,14 +1611,11 @@ def __iter__(self):
             self._bytes_read = 0
             self._started_at_0 = False
             self._read_to_eof = False
-            if self._fp.tell() == 0:
-                self._started_at_0 = True
-                self._iter_etag = hashlib.md5()
+            self._init_checks()
             while True:
                 chunk = self._fp.read(self._disk_chunk_size)
                 if chunk:
-                    if self._iter_etag:
-                        self._iter_etag.update(chunk)
+                    self._update_checks(chunk)
                     self._bytes_read += len(chunk)
                     if self._bytes_read - dropped_cache > DROP_CACHE_WINDOW:
                         self._drop_cache(self._fp.fileno(), dropped_cache,
@@ -2571,7 +2579,79 @@ def _hash_suffix(self, path, reclaim_age):
 
 
 class ECDiskFileReader(BaseDiskFileReader):
-    pass
+    def __init__(self, fp, data_file, obj_size, etag,
+                 disk_chunk_size, keep_cache_size, device_path, logger,
+                 quarantine_hook, use_splice, pipe_size, diskfile,
+                 keep_cache=False):
+        super(ECDiskFileReader, self).__init__(
+            fp, data_file, obj_size, etag,
+            disk_chunk_size, keep_cache_size, device_path, logger,
+            quarantine_hook, use_splice, pipe_size, diskfile, keep_cache)
+        self.frag_buf = None
+        self.frag_offset = 0
+        self.frag_size = self._diskfile.policy.fragment_size
+
+    def _init_checks(self):
+        super(ECDiskFileReader, self)._init_checks()
+        # for a multi-range GET this will be called at the start of each range;
+        # only initialise the frag_buf for reads starting at 0.
+        # TODO: reset frag buf to '' if tell() shows that start is on a frag
+        # boundary so that we check frags selected by a range not starting at 0
+        if self._started_at_0:
+            self.frag_buf = ''
+        else:
+            self.frag_buf = None
+
+    def _check_frag(self, frag):
+        if not frag:
+            return
+        if not isinstance(frag, six.binary_type):
+            # ECInvalidParameter can be returned if the frag violates the input
+            # format so for safety, check the input chunk if it's binary to
+            # avoid quarantining a valid fragment archive.
+            self._diskfile._logger.warn(
+                _('Unexpected fragment data type (not quarantined)'
+                  '%(datadir)s: %(type)s at offset 0x%(offset)x'),
+                {'datadir': self._diskfile._datadir,
+                 'type': type(frag),
+                 'offset': self.frag_offset})
+            return
+
+        try:
+            self._diskfile.policy.pyeclib_driver.get_metadata(frag)
+        except (ECInvalidFragmentMetadata, ECBadFragmentChecksum,
+                ECInvalidParameter):
+            # Any of these exceptions may be returned from ECDriver with a
+            # corrupted fragment.
+            msg = 'Invalid EC metadata at offset 0x%x' % self.frag_offset
+            self._quarantine(msg)
+            # We have to terminate the response iter with an exception but it
+            # can't be StopIteration, this will produce a STDERR traceback in
+            # eventlet.wsgi if you have eventlet_debug turned on; but any
+            # attempt to finish the iterator cleanly won't trigger the needful
+            # error handling cleanup - failing to do so, and yet still failing
+            # to deliver all promised bytes will hang the HTTP connection
+            raise DiskFileQuarantined(msg)
+        except ECDriverError as err:
+            self._diskfile._logger.warn(
+                _('Problem checking EC fragment %(datadir)s: %(err)s'),
+                {'datadir': self._diskfile._datadir, 'err': err})
+
+    def _update_checks(self, chunk):
+        super(ECDiskFileReader, self)._update_checks(chunk)
+        if self.frag_buf is not None:
+            self.frag_buf += chunk
+            cursor = 0
+            while len(self.frag_buf) >= cursor + self.frag_size:
+                self._check_frag(self.frag_buf[cursor:cursor + self.frag_size])
+                cursor += self.frag_size
+                self.frag_offset += self.frag_size
+            if cursor:
+                self.frag_buf = self.frag_buf[cursor:]
+
+    def _handle_close_quarantine(self):
+        super(ECDiskFileReader, self)._handle_close_quarantine()
+        self._check_frag(self.frag_buf)
 
 
 class ECDiskFileWriter(BaseDiskFileWriter):
diff --git a/test/unit/__init__.py b/test/unit/__init__.py
index a33ea08..ee2a262 100644
--- a/test/unit/__init__.py
+++ b/test/unit/__init__.py
@@ -1089,3 +1089,32 @@ def wrapper(*args, **kwargs):
             raise SkipTest('Requires O_TMPFILE support')
         return func(*args, **kwargs)
     return wrapper
+
+
+def encode_frag_archive_bodies(policy, body):
+    """
+    Given a stub body produce a list of complete frag_archive bodies as
+    strings in frag_index order.
+
+    :param policy: a StoragePolicy instance, with policy_type EC_POLICY
+    :param body: a string, the body to encode into frag archives
+
+    :returns: list of strings, the complete frag_archive bodies for the given
+              plaintext
+    """
+    segment_size = policy.ec_segment_size
+    # split up the body into buffers
+    chunks = [body[x:x + segment_size]
+              for x in range(0, len(body), segment_size)]
+    # encode the buffers into fragment payloads
+    fragment_payloads = []
+    for chunk in chunks:
+        fragments = policy.pyeclib_driver.encode(chunk)
+        if not fragments:
+            break
+        fragment_payloads.append(fragments)
+
+    # join up the fragment payloads per node
+    ec_archive_bodies = [''.join(frags)
+                         for frags in zip(*fragment_payloads)]
+    return ec_archive_bodies
diff --git a/test/unit/obj/common.py b/test/unit/obj/common.py
index 8cb618f..91a866e 100644
--- a/test/unit/obj/common.py
+++ b/test/unit/obj/common.py
@@ -13,6 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import hashlib
+import os
 import shutil
 import tempfile
 import unittest
@@ -42,6 +43,28 @@ def __init__(self, testdir, policy=None):
         self._diskfile_mgr = self._diskfile_router[policy]
 
 
+def write_diskfile(df, timestamp, data='test data', frag_index=None,
+                   commit=True, extra_metadata=None):
+    # Helper method to write some data and metadata to a diskfile.
+    # Optionally do not commit the diskfile
+    with df.create() as writer:
+        writer.write(data)
+        metadata = {
+            'ETag': hashlib.md5(data).hexdigest(),
+            'X-Timestamp': timestamp.internal,
+            'Content-Length': str(len(data)),
+        }
+        if extra_metadata:
+            metadata.update(extra_metadata)
+        if frag_index is not None:
+            metadata['X-Object-Sysmeta-Ec-Frag-Index'] = str(frag_index)
+        writer.put(metadata)
+        if commit:
+            writer.commit(timestamp)
+        # else: don't make it durable
+    return metadata
+
+
 class BaseTest(unittest.TestCase):
     def setUp(self):
         # daemon will be set in subclass setUp
@@ -64,20 +87,19 @@ def _make_diskfile(self, device='dev', partition='9',
         df = df_mgr.get_diskfile(
             device, partition, *object_parts, policy=policy,
             frag_index=frag_index)
-        content_length = len(body)
-        etag = hashlib.md5(body).hexdigest()
-        with df.create() as writer:
-            writer.write(body)
-            metadata = {
-                'X-Timestamp': timestamp.internal,
-                'Content-Length': str(content_length),
-                'ETag': etag,
-            }
-            if extra_metadata:
-                metadata.update(extra_metadata)
-            writer.put(metadata)
-            if commit:
-                writer.commit(timestamp)
+        write_diskfile(df, timestamp, data=body, extra_metadata=extra_metadata,
+                       commit=commit)
+
+        if commit:
+            # when we write and commit stub data, sanity check it's readable
+            # and not quarantined because of any validation check
+            with df.open():
+                self.assertEqual(''.join(df.reader()), body)
+            # sanity checks
+            listing = os.listdir(df._datadir)
+            self.assertTrue(listing)
+            for filename in listing:
+                self.assertTrue(filename.startswith(timestamp.internal))
         return df
 
     def _make_open_diskfile(self, device='dev', partition='9',
diff --git a/test/unit/obj/test_auditor.py b/test/unit/obj/test_auditor.py
index c76609e..afad56c 100644
--- a/test/unit/obj/test_auditor.py
+++ b/test/unit/obj/test_auditor.py
@@ -34,7 +34,7 @@
 from swift.common.utils import (
     mkdirs, normalize_timestamp, Timestamp, readconf)
 from swift.common.storage_policy import (
-    ECStoragePolicy, StoragePolicy, POLICIES)
+    ECStoragePolicy, StoragePolicy, POLICIES, EC_POLICY)
 
 
 _mocked_policies = [
@@ -155,6 +155,8 @@ def run_tests(disk_file):
             auditor_worker = auditor.AuditorWorker(self.conf, self.logger,
                                                    self.rcache, self.devices)
             data = b'0' * 1024
+            if disk_file.policy.policy_type == EC_POLICY:
+                data = disk_file.policy.pyeclib_driver.encode(data)[0]
             etag = md5()
             with disk_file.create() as writer:
                 writer.write(data)
@@ -226,6 +228,83 @@ def test_object_audit_diff_data(self):
                           policy=POLICIES.legacy))
         self.assertEqual(auditor_worker.quarantines, pre_quarantines + 1)
 
+    def test_object_audit_checks_EC_fragments(self):
+        disk_file = self.disk_file_ec
+
+        def do_test(data):
+            # create diskfile and set ETag and content-length to match the data
+            etag = md5(data).hexdigest()
+            timestamp = str(normalize_timestamp(time.time()))
+            with disk_file.create() as writer:
+                writer.write(data)
+                metadata = {
+                    'ETag': etag,
+                    'X-Timestamp': timestamp,
+                    'Content-Length': len(data),
+                }
+                writer.put(metadata)
+                writer.commit(Timestamp(timestamp))
+
+            auditor_worker = auditor.AuditorWorker(self.conf, FakeLogger(),
+                                                   self.rcache, self.devices)
+            self.assertEqual(0, auditor_worker.quarantines)  # sanity check
+            auditor_worker.object_audit(
+                AuditLocation(disk_file._datadir, 'sda', '0',
+                              policy=disk_file.policy))
+            return auditor_worker
+
+        # two good frags in an EC archive
+        frag_0 = disk_file.policy.pyeclib_driver.encode(
+            'x' * disk_file.policy.ec_segment_size)[0]
+        frag_1 = disk_file.policy.pyeclib_driver.encode(
+            'y' * disk_file.policy.ec_segment_size)[0]
+        data = frag_0 + frag_1
+        auditor_worker = do_test(data)
+        self.assertEqual(0, auditor_worker.quarantines)
+        self.assertFalse(auditor_worker.logger.get_lines_for_level('error'))
+
+        # corrupt second frag headers
+        corrupt_frag_1 = 'blah' * 16 + frag_1[64:]
+        data = frag_0 + corrupt_frag_1
+        auditor_worker = do_test(data)
+        self.assertEqual(1, auditor_worker.quarantines)
+        log_lines = auditor_worker.logger.get_lines_for_level('error')
+        self.assertIn('failed audit and was quarantined: '
+                      'Invalid EC metadata at offset 0x%x' %
+                      len(frag_0),
+                      log_lines[0])
+
+        # dangling extra corrupt frag data
+        data = frag_0 + frag_1 + 'wtf' * 100
+        auditor_worker = do_test(data)
+        self.assertEqual(1, auditor_worker.quarantines)
+        log_lines = auditor_worker.logger.get_lines_for_level('error')
+        self.assertIn('failed audit and was quarantined: '
+                      'Invalid EC metadata at offset 0x%x' %
+                      len(frag_0 + frag_1),
+                      log_lines[0])
+
+        # simulate bug https://bugs.launchpad.net/bugs/1631144 by writing start
+        # of an ssync subrequest into the diskfile
+        data = (
+            b'PUT /a/c/o\r\n' +
+            b'Content-Length: 999\r\n' +
+            b'Content-Type: image/jpeg\r\n' +
+            b'X-Object-Sysmeta-Ec-Content-Length: 1024\r\n' +
+            b'X-Object-Sysmeta-Ec-Etag: 1234bff7eb767cc6d19627c6b6f9edef\r\n' +
+            b'X-Object-Sysmeta-Ec-Frag-Index: 1\r\n' +
+            b'X-Object-Sysmeta-Ec-Scheme: ' + DEFAULT_TEST_EC_TYPE + '\r\n' +
+            b'X-Object-Sysmeta-Ec-Segment-Size: 1048576\r\n' +
+            b'X-Timestamp: 1471512345.17333\r\n\r\n'
+        )
+        data += frag_0[:disk_file.policy.fragment_size - len(data)]
+        auditor_worker = do_test(data)
+        self.assertEqual(1, auditor_worker.quarantines)
+        log_lines = auditor_worker.logger.get_lines_for_level('error')
+        self.assertIn('failed audit and was quarantined: '
+                      'Invalid EC metadata at offset 0x0',
+                      log_lines[0])
+
     def test_object_audit_no_meta(self):
         timestamp = str(normalize_timestamp(time.time()))
         path = os.path.join(self.disk_file._datadir, timestamp + '.data')
diff --git a/test/unit/obj/test_diskfile.py b/test/unit/obj/test_diskfile.py
index c241f91..c984c1d 100644
--- a/test/unit/obj/test_diskfile.py
+++ b/test/unit/obj/test_diskfile.py
@@ -36,13 +36,14 @@
 from hashlib import md5
 from contextlib import closing, contextmanager
 from gzip import GzipFile
+import pyeclib.ec_iface
 
 from eventlet import hubs, timeout, tpool
 from swift.obj.diskfile import MD5_OF_EMPTY_STRING, update_auditor_status
 from test.unit import (FakeLogger, mock as unit_mock, temptree,
                        patch_policies, debug_logger, EMPTY_ETAG,
                        make_timestamp_iter, DEFAULT_TEST_EC_TYPE,
-                       requires_o_tmpfile_support)
+                       requires_o_tmpfile_support, encode_frag_archive_bodies)
 from nose import SkipTest
 from swift.obj import diskfile
 from swift.common import utils
@@ -57,7 +58,7 @@
 from swift.common.storage_policy import (
     POLICIES, get_policy_string, StoragePolicy, ECStoragePolicy,
     BaseStoragePolicy, REPL_POLICY, EC_POLICY)
-
+from test.unit.obj.common import write_diskfile
 
 test_policies = [
     StoragePolicy(0, name='zero', is_default=True),
@@ -2317,6 +2318,10 @@ def _create_test_file(self, data, timestamp=None, metadata=None,
         if timestamp is None:
             timestamp = time()
         timestamp = Timestamp(timestamp)
+
+        if df.policy.policy_type == EC_POLICY:
+            data = encode_frag_archive_bodies(df.policy, data)[df._frag_index]
+
         with df.create() as writer:
             new_metadata = {
                 'ETag': md5(data).hexdigest(),
@@ -2328,7 +2333,7 @@ def _create_test_file(self, data, timestamp=None, metadata=None,
             writer.put(new_metadata)
             writer.commit(timestamp)
         df.open()
-        return df
+        return df, data
 
     def test_get_dev_path(self):
         self.df_mgr.devices = '/srv'
@@ -2388,7 +2393,8 @@ def test_open_not_expired(self):
 
     def test_get_metadata(self):
         timestamp = self.ts().internal
-        df = self._create_test_file('1234567890', timestamp=timestamp)
+        df, df_data = self._create_test_file('1234567890',
+                                             timestamp=timestamp)
         md = df.get_metadata()
         self.assertEqual(md['X-Timestamp'], timestamp)
 
@@ -2421,8 +2427,8 @@ def test_get_datafile_metadata(self):
         ts_data = next(ts_iter)
         metadata = {'X-Object-Meta-Test': 'test1',
                     'X-Object-Sysmeta-Test': 'test1'}
-        df = self._create_test_file(body, timestamp=ts_data.internal,
-                                    metadata=metadata)
+        df, df_data = self._create_test_file(body, timestamp=ts_data.internal,
+                                             metadata=metadata)
         expected = df.get_metadata()
         ts_meta = next(ts_iter)
         df.write_metadata({'X-Timestamp': ts_meta.internal,
@@ -2445,8 +2451,8 @@ def test_get_metafile_metadata(self):
         ts_data = next(ts_iter)
         metadata = {'X-Object-Meta-Test': 'test1',
                     'X-Object-Sysmeta-Test': 'test1'}
-        df = self._create_test_file(body, timestamp=ts_data.internal,
-                                    metadata=metadata)
+        df, df_data = self._create_test_file(body, timestamp=ts_data.internal,
+                                             metadata=metadata)
         self.assertIsNone(df.get_metafile_metadata())
 
         # now create a meta file
@@ -2477,7 +2483,12 @@ def test_disk_file_default_disallowed_metadata(self):
         df = self._get_open_disk_file(ts=self.ts().internal,
                                       extra_metadata=orig_metadata)
         with df.open():
-            self.assertEqual('1024', df._metadata['Content-Length'])
+            if df.policy.policy_type == EC_POLICY:
+                expected = df.policy.pyeclib_driver.get_segment_info(
+                    1024, df.policy.ec_segment_size)['fragment_size']
+            else:
+                expected = 1024
+            self.assertEqual(str(expected), df._metadata['Content-Length'])
         # write some new metadata (fast POST, don't send orig meta, at t0+1)
         df = self._simple_get_diskfile()
         df.write_metadata({'X-Timestamp': self.ts().internal,
@@ -2502,7 +2513,12 @@ def test_disk_file_preserves_sysmeta(self):
         df = self._get_open_disk_file(ts=self.ts().internal,
                                       extra_metadata=orig_metadata)
         with df.open():
-            self.assertEqual('1024', df._metadata['Content-Length'])
+            if df.policy.policy_type == EC_POLICY:
+                expected = df.policy.pyeclib_driver.get_segment_info(
+                    1024, df.policy.ec_segment_size)['fragment_size']
+            else:
+                expected = 1024
+            self.assertEqual(str(expected), df._metadata['Content-Length'])
         # write some new metadata (fast POST, don't send orig meta, at t0+1s)
         df = self._simple_get_diskfile()
         df.write_metadata({'X-Timestamp': self.ts().internal,
@@ -2516,14 +2532,14 @@ def test_disk_file_preserves_sysmeta(self):
             self.assertEqual('Value1', df._metadata['X-Object-Sysmeta-Key1'])
 
     def test_disk_file_reader_iter(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
-        self.assertEqual(''.join(reader), '1234567890')
+        self.assertEqual(''.join(reader), df_data)
         self.assertEqual(quarantine_msgs, [])
 
     def test_disk_file_reader_iter_w_quarantine(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
 
         def raise_dfq(m):
             raise DiskFileQuarantined(m)
@@ -2533,94 +2549,96 @@ def raise_dfq(m):
         self.assertRaises(DiskFileQuarantined, ''.join, reader)
 
     def test_disk_file_app_iter_corners(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         self.assertEqual(''.join(reader.app_iter_range(0, None)),
-                         '1234567890')
+                         df_data)
         self.assertEqual(quarantine_msgs, [])
         df = self._simple_get_diskfile()
         with df.open():
             reader = df.reader()
-            self.assertEqual(''.join(reader.app_iter_range(5, None)), '67890')
+            self.assertEqual(''.join(reader.app_iter_range(5, None)),
+                             df_data[5:])
 
     def test_disk_file_app_iter_range_w_none(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         self.assertEqual(''.join(reader.app_iter_range(None, None)),
-                         '1234567890')
+                         df_data)
         self.assertEqual(quarantine_msgs, [])
 
     def test_disk_file_app_iter_partial_closes(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_range(0, 5)
-        self.assertEqual(''.join(it), '12345')
+        self.assertEqual(''.join(it), df_data[:5])
         self.assertEqual(quarantine_msgs, [])
         self.assertTrue(reader._fp is None)
 
     def test_disk_file_app_iter_ranges(self):
-        df = self._create_test_file('012345678911234567892123456789')
+        df, df_data = self._create_test_file('012345678911234567892123456789')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_ranges([(0, 10), (10, 20), (20, 30)],
                                     'plain/text',
-                                    '\r\n--someheader\r\n', 30)
+                                    '\r\n--someheader\r\n', len(df_data))
         value = ''.join(it)
-        self.assertIn('0123456789', value)
-        self.assertIn('1123456789', value)
-        self.assertIn('2123456789', value)
+        self.assertIn(df_data[:10], value)
+        self.assertIn(df_data[10:20], value)
+        self.assertIn(df_data[20:30], value)
         self.assertEqual(quarantine_msgs, [])
 
     def test_disk_file_app_iter_ranges_w_quarantine(self):
-        df = self._create_test_file('012345678911234567892123456789')
+        df, df_data = self._create_test_file('012345678911234567892123456789')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
+        self.assertEqual(len(df_data), reader._obj_size)  # sanity check
         reader._obj_size += 1
-        it = reader.app_iter_ranges([(0, 30)],
+        it = reader.app_iter_ranges([(0, len(df_data))],
                                     'plain/text',
-                                    '\r\n--someheader\r\n', 30)
+                                    '\r\n--someheader\r\n', len(df_data))
         value = ''.join(it)
-        self.assertIn('0123456789', value)
-        self.assertIn('1123456789', value)
-        self.assertIn('2123456789', value)
+        self.assertIn(df_data, value)
         self.assertEqual(quarantine_msgs,
-                         ["Bytes read: 30, does not match metadata: 31"])
+                         ["Bytes read: %s, does not match metadata: %s" %
+                          (len(df_data), len(df_data) + 1)])
 
     def test_disk_file_app_iter_ranges_w_no_etag_quarantine(self):
-        df = self._create_test_file('012345678911234567892123456789')
+        df, df_data = self._create_test_file('012345678911234567892123456789')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_ranges([(0, 10)],
                                     'plain/text',
-                                    '\r\n--someheader\r\n', 30)
+                                    '\r\n--someheader\r\n', len(df_data))
         value = ''.join(it)
-        self.assertIn('0123456789', value)
+        self.assertIn(df_data[:10], value)
         self.assertEqual(quarantine_msgs, [])
 
     def test_disk_file_app_iter_ranges_edges(self):
-        df = self._create_test_file('012345678911234567892123456789')
+        df, df_data = self._create_test_file('012345678911234567892123456789')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_ranges([(3, 10), (0, 2)], 'application/whatever',
-                                    '\r\n--someheader\r\n', 30)
+                                    '\r\n--someheader\r\n', len(df_data))
         value = ''.join(it)
-        self.assertIn('3456789', value)
-        self.assertIn('01', value)
+        self.assertIn(df_data[3:10], value)
+        self.assertIn(df_data[:2], value)
         self.assertEqual(quarantine_msgs, [])
 
     def test_disk_file_large_app_iter_ranges(self):
         # This test case is to make sure that the disk file app_iter_ranges
         # method all the paths being tested.
         long_str = '01234567890' * 65536
-        target_strs = ['3456789', long_str[0:65590]]
-        df = self._create_test_file(long_str)
+        df, df_data = self._create_test_file(long_str)
+        target_strs = [df_data[3:10], df_data[0:65590]]
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_ranges([(3, 10), (0, 65590)], 'plain/text',
-                                    '5e816ff8b8b8e9a5d355497e5d9e0301', 655360)
+                                    '5e816ff8b8b8e9a5d355497e5d9e0301',
+                                    len(df_data))
 
         # The produced string actually missing the MIME headers
         # need to add these headers to make it as real MIME message.
@@ -2641,11 +2659,11 @@ def test_disk_file_app_iter_ranges_empty(self):
         # This test case tests when empty value passed into app_iter_ranges
         # When ranges passed into the method is either empty array or None,
         # this method will yield empty string
-        df = self._create_test_file('012345678911234567892123456789')
+        df, df_data = self._create_test_file('012345678911234567892123456789')
         quarantine_msgs = []
         reader = df.reader(_quarantine_hook=quarantine_msgs.append)
         it = reader.app_iter_ranges([], 'application/whatever',
-                                    '\r\n--someheader\r\n', 100)
+                                    '\r\n--someheader\r\n', len(df_data))
         self.assertEqual(''.join(it), '')
 
         df = self._simple_get_diskfile()
@@ -2676,6 +2694,13 @@ def _get_open_disk_file(self, invalid_type=None, obj_name='o', fsize=1024,
         df = self._simple_get_diskfile(obj=obj_name, policy=policy,
                                        frag_index=frag_index)
         data = data or '0' * fsize
+        if policy.policy_type == EC_POLICY:
+            archives = encode_frag_archive_bodies(policy, data)
+            try:
+                data = archives[df._frag_index]
+            except IndexError:
+                data = archives[0]
+
         etag = md5()
         if ts:
             timestamp = Timestamp(ts)
@@ -2782,7 +2807,7 @@ def test_keep_cache(self):
                 pass
             self.assertFalse(boo.called)
 
-        df = self._get_open_disk_file(fsize=5 * 1024, csize=256)
+        df = self._get_open_disk_file(fsize=50 * 1024, csize=256)
         with mock.patch("swift.obj.diskfile.drop_buffer_cache") as goo:
             for _ in df.reader(keep_cache=True):
                 pass
@@ -2974,8 +2999,8 @@ def bad_fstat(fd):
                 self._get_open_disk_file)
 
     def test_quarantine_hashdir_not_a_directory(self):
-        df = self._create_test_file('1234567890', account="abc",
-                                    container='123', obj='xyz')
+        df, df_data = self._create_test_file('1234567890', account="abc",
+                                             container='123', obj='xyz')
         hashdir = df._datadir
         rmtree(hashdir)
         with open(hashdir, 'w'):
@@ -3068,7 +3093,7 @@ def test_create_close_oserror(self):
                 pass
 
     def test_write_metadata(self):
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         file_count = len(os.listdir(df._datadir))
         timestamp = Timestamp(time()).internal
         metadata = {'X-Timestamp': timestamp, 'X-Object-Meta-test': 'data'}
@@ -3080,7 +3105,7 @@ def test_write_metadata(self):
 
     def test_write_metadata_with_content_type(self):
         # if metadata has content-type then its time should be in file name
-        df = self._create_test_file('1234567890')
+        df, df_data = self._create_test_file('1234567890')
         file_count = len(os.listdir(df._datadir))
         timestamp = Timestamp(time())
         metadata = {'X-Timestamp': timestamp.internal,
@@ -3097,7 +3122,8 @@ def test_write_metadata_with_content_type(self):
     def test_write_metadata_with_older_content_type(self):
         # if metadata has content-type then its time should be in file name
         ts_iter = make_timestamp_iter()
-        df = self._create_test_file('1234567890', timestamp=ts_iter.next())
+        df, df_data = self._create_test_file('1234567890',
+                                             timestamp=ts_iter.next())
         file_count = len(os.listdir(df._datadir))
         timestamp = ts_iter.next()
         timestamp2 = ts_iter.next()
@@ -3117,7 +3143,8 @@ def test_write_metadata_with_content_type_removes_same_time_meta(self):
         # a meta file without content-type should be cleaned up in favour of
         # a meta file at same time with content-type
         ts_iter = make_timestamp_iter()
-        df = self._create_test_file('1234567890', timestamp=ts_iter.next())
+        df, df_data = self._create_test_file('1234567890',
+                                             timestamp=ts_iter.next())
         file_count = len(os.listdir(df._datadir))
         timestamp = ts_iter.next()
         timestamp2 = ts_iter.next()
@@ -3142,7 +3169,8 @@ def test_write_metadata_with_content_type_removes_multiple_metas(self):
         # file with content-type should be cleaned up in favour of a meta file
         # at newer time with content-type
         ts_iter = make_timestamp_iter()
-        df = self._create_test_file('1234567890', timestamp=ts_iter.next())
+        df, df_data = self._create_test_file('1234567890',
+                                             timestamp=ts_iter.next())
         file_count = len(os.listdir(df._datadir))
         timestamp = ts_iter.next()
         timestamp2 = ts_iter.next()
@@ -3421,9 +3449,10 @@ def test_open_deleted_with_corrupt_tombstone(self):
         self.assertFalse(os.path.exists(ts_fullpath))
 
     def test_from_audit_location(self):
-        hashdir = self._create_test_file(
+        df, df_data = self._create_test_file(
             'blah blah',
-            account='three', container='blind', obj='mice')._datadir
+            account='three', container='blind', obj='mice')
+        hashdir = df._datadir
         df = self.df_mgr.get_diskfile_from_audit_location(
             diskfile.AuditLocation(hashdir, self.existing_device, '0',
                                    policy=POLICIES.default))
@@ -3431,9 +3460,10 @@ def test_from_audit_location(self):
         self.assertEqual(df._name, '/three/blind/mice')
 
     def test_from_audit_location_with_mismatched_hash(self):
-        hashdir = self._create_test_file(
+        df, df_data = self._create_test_file(
             'blah blah',
-            account='this', container='is', obj='right')._datadir
+            account='this', container='is', obj='right')
+        hashdir = df._datadir
         datafilename = [f for f in os.listdir(hashdir)
                         if f.endswith('.data')][0]
         datafile = os.path.join(hashdir, datafilename)
@@ -3678,7 +3708,12 @@ def test_diskfile_content_length(self):
         self._get_open_disk_file()
         df = self._simple_get_diskfile()
         with df.open():
-            self.assertEqual(df.content_length, 1024)
+            if df.policy.policy_type == EC_POLICY:
+                expected = df.policy.pyeclib_driver.get_segment_info(
+                    1024, df.policy.ec_segment_size)['fragment_size']
+            else:
+                expected = 1024
+            self.assertEqual(df.content_length, expected)
 
     def test_diskfile_timestamp_not_open(self):
         df = self._simple_get_diskfile()
@@ -3829,14 +3864,18 @@ def test_zero_copy_cache_dropping(self):
         self.conf['keep_cache_size'] = 16384
         self.conf['disk_chunk_size'] = 4096
 
-        df = self._get_open_disk_file(fsize=16385)
+        df = self._get_open_disk_file(fsize=163840)
         reader = df.reader()
         self.assertTrue(reader.can_zero_copy_send())
         with mock.patch("swift.obj.diskfile.drop_buffer_cache") as dbc:
             with mock.patch("swift.obj.diskfile.DROP_CACHE_WINDOW", 4095):
                 with open('/dev/null', 'w') as devnull:
                     reader.zero_copy_send(devnull.fileno())
-                self.assertEqual(len(dbc.mock_calls), 5)
+                if df.policy.policy_type == EC_POLICY:
+                    expected = 4 + 1
+                else:
+                    expected = (4 * 10) + 1
+                self.assertEqual(len(dbc.mock_calls), expected)
 
     def test_zero_copy_turns_off_when_md5_sockets_not_supported(self):
         if not self._system_can_zero_copy():
@@ -4901,6 +4940,156 @@ def test_open_with_bad_fragment_preferences(self):
             except DiskFileError as e:
                 self.assertIn('frag_prefs', str(e))
 
+    def test_disk_file_app_iter_ranges_checks_only_aligned_frag_data(self):
+        policy = POLICIES.default
+        frag_size = policy.fragment_size
+        # make sure there are two fragment size worth of data on disk
+        data = 'ab' * policy.ec_segment_size
+        df, df_data = self._create_test_file(data)
+        quarantine_msgs = []
+        reader = df.reader(_quarantine_hook=quarantine_msgs.append)
+        # each range uses a fresh reader app_iter_range which triggers a disk
+        # read at the range offset - make sure each of those disk reads will
+        # fetch an amount of data from disk that is greater than but not equal
+        # to a fragment size
+        reader._disk_chunk_size = int(frag_size * 1.5)
+        with mock.patch.object(
+                reader._diskfile.policy.pyeclib_driver, 'get_metadata')\
+                as mock_get_metadata:
+            it = reader.app_iter_ranges(
+                [(0, 10), (10, 20),
+                 (frag_size + 20, frag_size + 30)],
+                'plain/text', '\r\n--someheader\r\n', len(df_data))
+            value = ''.join(it)
+        # check that only first range which starts at 0 triggers a frag check
+        self.assertEqual(1, mock_get_metadata.call_count)
+        self.assertIn(df_data[:10], value)
+        self.assertIn(df_data[10:20], value)
+        self.assertIn(df_data[frag_size + 20:frag_size + 30], value)
+        self.assertEqual(quarantine_msgs, [])
+
+    def test_reader_quarantines_corrupted_ec_archive(self):
+        # This has same purpose as
+        # TestAuditor.test_object_audit_checks_EC_fragments just making
+        # sure that checks happen in DiskFileReader layer.
+        policy = POLICIES.default
+        df, df_data = self._create_test_file('x' * policy.ec_segment_size,
+                                             timestamp=self.ts())
+
+        def do_test(corrupted_frag_body, expected_offset, expected_read):
+            # expected_offset is offset at which corruption should be reported
+            # expected_read is number of bytes that should be read before the
+            # exception is raised
+            ts = self.ts()
+            write_diskfile(df, ts, corrupted_frag_body)
+
+            # at the open for the diskfile, no error occurred
+            # reading first corrupt frag is sufficient to detect the corruption
+            df.open()
+            with self.assertRaises(DiskFileQuarantined) as cm:
+                reader = df.reader()
+                reader._disk_chunk_size = int(policy.fragment_size)
+                bytes_read = 0
+                for chunk in reader:
+                    bytes_read += len(chunk)
+
+            with self.assertRaises(DiskFileNotExist):
+                df.open()
+
+            self.assertEqual(expected_read, bytes_read)
+            self.assertEqual('Invalid EC metadata at offset 0x%x' %
+                             expected_offset, cm.exception.message)
+
+        # TODO with liberasurecode < 1.2.0 the EC metadata verification checks
+        # only the magic number at offset 59 bytes into the frag so we'll
+        # corrupt up to and including that. Once liberasurecode >= 1.2.0 is
+        # required we should be able to reduce the corruption length.
+        corruption_length = 64
+        # corrupted first frag can be detected
+        corrupted_frag_body = (' ' * corruption_length +
+                               df_data[corruption_length:])
+        do_test(corrupted_frag_body, 0, 0)
+
+        # corrupted the second frag can be also detected
+        corrupted_frag_body = (df_data + ' ' * corruption_length +
+                               df_data[corruption_length:])
+        do_test(corrupted_frag_body, len(df_data), len(df_data))
+
+        # if the second frag is shorter than frag size then corruption is
+        # detected when the reader is closed
+        corrupted_frag_body = (df_data + ' ' * corruption_length +
+                               df_data[corruption_length:-10])
+        do_test(corrupted_frag_body, len(df_data), len(corrupted_frag_body))
+
+    def test_reader_ec_exception_causes_quarantine(self):
+        policy = POLICIES.default
+
+        def do_test(exception):
+            df, df_data = self._create_test_file('x' * policy.ec_segment_size,
+                                                 timestamp=self.ts())
+            df.manager.logger.clear()
+
+            with mock.patch.object(df.policy.pyeclib_driver, 'get_metadata',
+                                   side_effect=exception):
+                df.open()
+                with self.assertRaises(DiskFileQuarantined) as cm:
+                    for chunk in df.reader():
+                        pass
+
+            with self.assertRaises(DiskFileNotExist):
+                df.open()
+
+            self.assertEqual('Invalid EC metadata at offset 0x0',
+                             cm.exception.message)
+            log_lines = df.manager.logger.get_lines_for_level('warning')
+            self.assertIn('Quarantined object', log_lines[0])
+            self.assertIn('Invalid EC metadata at offset 0x0', log_lines[0])
+
+        do_test(pyeclib.ec_iface.ECInvalidFragmentMetadata('testing'))
+        do_test(pyeclib.ec_iface.ECBadFragmentChecksum('testing'))
+        do_test(pyeclib.ec_iface.ECInvalidParameter('testing'))
+
+    def test_reader_ec_exception_does_not_cause_quarantine(self):
+        # ECDriverError should not cause quarantine, only certain subclasses
+        policy = POLICIES.default
+
+        df, df_data = self._create_test_file('x' * policy.ec_segment_size,
+                                             timestamp=self.ts())
+
+        with mock.patch.object(
+                df.policy.pyeclib_driver, 'get_metadata',
+                side_effect=pyeclib.ec_iface.ECDriverError('testing')):
+            df.open()
+            read_data = ''.join([d for d in df.reader()])
+        self.assertEqual(df_data, read_data)
+        log_lines = df.manager.logger.get_lines_for_level('warning')
+        self.assertIn('Problem checking EC fragment', log_lines[0])
+
+        df.open()  # not quarantined
+
+    def test_reader_frag_check_does_not_quarantine_if_its_not_binary(self):
+        # This may look weird but for super-safety, check the
+        # ECDiskFileReader._frag_check doesn't quarantine when non-binary
+        # type chunk incomming (that would occurre only from coding bug)
+        policy = POLICIES.default
+
+        df, df_data = self._create_test_file('x' * policy.ec_segment_size,
+                                             timestamp=self.ts())
+        df.open()
+        for invalid_type_chunk in (None, [], [[]], 1):
+            reader = df.reader()
+            reader._check_frag(invalid_type_chunk)
+
+        # None and [] are just skipped and [[]] and 1 are detected as invalid
+        # chunks
+        log_lines = df.manager.logger.get_lines_for_level('warning')
+        self.assertEqual(2, len(log_lines))
+        for log_line in log_lines:
+            self.assertIn(
+                'Unexpected fragment data type (not quarantined)', log_line)
+
+        df.open()  # not quarantined
+
 
 @patch_policies(with_ec_default=True)
 class TestSuffixHashes(unittest.TestCase):
diff --git a/test/unit/obj/test_server.py b/test/unit/obj/test_server.py
index fab3a92..3cfa408 100755
--- a/test/unit/obj/test_server.py
+++ b/test/unit/obj/test_server.py
@@ -45,7 +45,8 @@
 from swift.common.http import is_success
 from test.unit import FakeLogger, debug_logger, mocked_http_conn, \
     make_timestamp_iter, DEFAULT_TEST_EC_TYPE
-from test.unit import connect_tcp, readuntil2crlfs, patch_policies
+from test.unit import connect_tcp, readuntil2crlfs, patch_policies, \
+    encode_frag_archive_bodies
 from swift.obj import server as object_server
 from swift.obj import updater
 from swift.obj import diskfile
@@ -59,7 +60,8 @@
 from swift.common.splice import splice
 from swift.common.storage_policy import (StoragePolicy, ECStoragePolicy,
                                          POLICIES, EC_POLICY)
-from swift.common.exceptions import DiskFileDeviceUnavailable, DiskFileNoSpace
+from swift.common.exceptions import DiskFileDeviceUnavailable, \
+    DiskFileNoSpace, DiskFileQuarantined
 
 
 def mock_time(*args, **kwargs):
@@ -121,6 +123,7 @@ def setUp(self):
 
         self.logger = debug_logger('test-object-controller')
         self.ts = make_timestamp_iter()
+        self.ec_policies = [p for p in POLICIES if p.policy_type == EC_POLICY]
 
     def tearDown(self):
         """Tear down for testing swift.object.server.ObjectController"""
@@ -2357,6 +2360,72 @@ def read(self, amt=None):
                 resp = req.get_response(self.object_controller)
         self.assertEqual(resp.status_int, 201)
 
+    def test_EC_GET_PUT_data(self):
+        for policy in self.ec_policies:
+            raw_data = ('VERIFY' * policy.ec_segment_size)[:-432]
+            frag_archives = encode_frag_archive_bodies(policy, raw_data)
+            frag_index = random.randint(0, len(frag_archives) - 1)
+            # put EC frag archive
+            req = Request.blank('/sda1/p/a/c/o', method='PUT', headers={
+                'X-Timestamp': next(self.ts).internal,
+                'Content-Type': 'application/verify',
+                'Content-Length': len(frag_archives[frag_index]),
+                'X-Object-Sysmeta-Ec-Frag-Index': frag_index,
+                'X-Backend-Storage-Policy-Index': int(policy),
+            })
+            req.body = frag_archives[frag_index]
+            resp = req.get_response(self.object_controller)
+            self.assertEqual(resp.status_int, 201)
+
+            # get EC frag archive
+            req = Request.blank('/sda1/p/a/c/o', headers={
+                'X-Backend-Storage-Policy-Index': int(policy),
+            })
+            resp = req.get_response(self.object_controller)
+            self.assertEqual(resp.status_int, 200)
+            self.assertEqual(resp.body, frag_archives[frag_index])
+
+    def test_EC_GET_quarantine_invalid_frag_archive(self):
+        policy = random.choice(self.ec_policies)
+        raw_data = ('VERIFY' * policy.ec_segment_size)[:-432]
+        frag_archives = encode_frag_archive_bodies(policy, raw_data)
+        frag_index = random.randint(0, len(frag_archives) - 1)
+        content_length = len(frag_archives[frag_index])
+        # put EC frag archive
+        req = Request.blank('/sda1/p/a/c/o', method='PUT', headers={
+            'X-Timestamp': next(self.ts).internal,
+            'Content-Type': 'application/verify',
+            'Content-Length': content_length,
+            'X-Object-Sysmeta-Ec-Frag-Index': frag_index,
+            'X-Backend-Storage-Policy-Index': int(policy),
+        })
+        corrupt = 'garbage' + frag_archives[frag_index]
+        req.body = corrupt[:content_length]
+        resp = req.get_response(self.object_controller)
+        self.assertEqual(resp.status_int, 201)
+
+        # get EC frag archive
+        req = Request.blank('/sda1/p/a/c/o', headers={
+            'X-Backend-Storage-Policy-Index': int(policy),
+        })
+        resp = req.get_response(self.object_controller)
+        self.assertEqual(resp.status_int, 200)
+
+        with self.assertRaises(DiskFileQuarantined) as ctx:
+            resp.body
+        self.assertIn("Invalid EC metadata", str(ctx.exception))
+
+        # nothing is logged on *our* loggers
+        errors = self.object_controller.logger.get_lines_for_level('error')
+        self.assertEqual(errors, [])
+
+        # get EC frag archive - it's gone
+        req = Request.blank('/sda1/p/a/c/o', headers={
+            'X-Backend-Storage-Policy-Index': int(policy),
+        })
+        resp = req.get_response(self.object_controller)
+        self.assertEqual(resp.status_int, 404)
+
     def test_PUT_ssync_multi_frag(self):
         timestamp = utils.Timestamp(time()).internal
 
@@ -3200,22 +3269,37 @@ def test_HEAD_if_unmodified_since(self):
         resp = req.get_response(self.object_controller)
         self.assertEqual(resp.status_int, 412)
 
+    def assertECBodyEqual(self, resp, expected):
+        # we pull the policy index from the request environ since it seems to
+        # be missing from the response headers
+        policy_index = int(
+            resp.request.headers['X-Backend-Storage-Policy-Index'])
+        policy = POLICIES[policy_index]
+        frags = encode_frag_archive_bodies(policy, expected)
+        frag_index = int(resp.headers['X-Object-Sysmeta-Ec-Frag-Index'])
+        self.assertEqual(resp.body, frags[frag_index])
+
     def _create_ondisk_fragments(self, policy):
         # Create some on disk files...
         ts_iter = make_timestamp_iter()
 
         # PUT at ts_0
         ts_0 = next(ts_iter)
+        body = 'OLDER'
         headers = {'X-Timestamp': ts_0.internal,
                    'Content-Length': '5',
                    'Content-Type': 'application/octet-stream',
                    'X-Backend-Storage-Policy-Index': int(policy)}
         if policy.policy_type == EC_POLICY:
-            headers['X-Object-Sysmeta-Ec-Frag-Index'] = '0'
+            body = encode_frag_archive_bodies(policy, body)[0]
+            headers.update({
+                'X-Object-Sysmeta-Ec-Frag-Index': '0',
+                'Content-Length': len(body),
+            })
         req = Request.blank('/sda1/p/a/c/o',
                             environ={'REQUEST_METHOD': 'PUT'},
                             headers=headers)
-        req.body = 'OLDER'
+        req.body = body
         resp = req.get_response(self.object_controller)
         self.assertEqual(resp.status_int, 201)
 
@@ -3232,16 +3316,21 @@ def _create_ondisk_fragments(self, policy):
 
         # PUT again at ts_2 but without a .durable file
         ts_2 = next(ts_iter)
+        body = 'NEWER'
         headers = {'X-Timestamp': ts_2.internal,
                    'Content-Length': '5',
                    'Content-Type': 'application/octet-stream',
                    'X-Backend-Storage-Policy-Index': int(policy)}
         if policy.policy_type == EC_POLICY:
-            headers['X-Object-Sysmeta-Ec-Frag-Index'] = '2'
+            body = encode_frag_archive_bodies(policy, body)[2]
+            headers.update({
+                'X-Object-Sysmeta-Ec-Frag-Index': '2',
+                'Content-Length': len(body),
+            })
         req = Request.blank('/sda1/p/a/c/o',
                             environ={'REQUEST_METHOD': 'PUT'},
                             headers=headers)
-        req.body = 'NEWER'
+        req.body = body
         # patch the commit method to do nothing so EC object gets
         # no .durable file
         with mock.patch('swift.obj.diskfile.ECDiskFileWriter.commit'):
@@ -3287,7 +3376,7 @@ def _assert_repl_data_at_ts_2():
 
             if policy.policy_type == EC_POLICY:
                 _assert_frag_0_at_ts_0(resp)
-                self.assertEqual(resp.body, 'OLDER')
+                self.assertECBodyEqual(resp, 'OLDER')
             else:
                 _assert_repl_data_at_ts_2()
                 self.assertEqual(resp.body, 'NEWER')
@@ -3295,6 +3384,7 @@ def _assert_repl_data_at_ts_2():
             req = Request.blank('/sda1/p/a/c/o', headers=headers,
                                 environ={'REQUEST_METHOD': 'HEAD'})
             resp = req.get_response(self.object_controller)
+            self.assertEqual(resp.status_int, 200)
             if policy.policy_type == EC_POLICY:
                 _assert_frag_0_at_ts_0(resp)
             else:
@@ -3311,7 +3401,7 @@ def _assert_repl_data_at_ts_2():
 
             if policy.policy_type == EC_POLICY:
                 _assert_frag_0_at_ts_0(resp)
-                self.assertEqual(resp.body, 'OLDER')
+                self.assertECBodyEqual(resp, 'OLDER')
             else:
                 _assert_repl_data_at_ts_2()
                 self.assertEqual(resp.body, 'NEWER')
@@ -3350,9 +3440,10 @@ def _assert_frag_2_at_ts_2(resp):
 
             if policy.policy_type == EC_POLICY:
                 _assert_frag_2_at_ts_2(resp)
+                self.assertECBodyEqual(resp, 'NEWER')
             else:
                 _assert_repl_data_at_ts_2()
-            self.assertEqual(resp.body, 'NEWER')
+                self.assertEqual(resp.body, 'NEWER')
 
             req = Request.blank('/sda1/p/a/c/o', headers=headers,
                                 environ={'REQUEST_METHOD': 'HEAD'})
@@ -3374,9 +3465,10 @@ def _assert_frag_2_at_ts_2(resp):
             resp = req.get_response(self.object_controller)
             if policy.policy_type == EC_POLICY:
                 _assert_frag_2_at_ts_2(resp)
+                self.assertECBodyEqual(resp, 'NEWER')
             else:
                 _assert_repl_data_at_ts_2()
-            self.assertEqual(resp.body, 'NEWER')
+                self.assertEqual(resp.body, 'NEWER')
 
             req = Request.blank('/sda1/p/a/c/o', headers=headers,
                                 environ={'REQUEST_METHOD': 'HEAD'})
@@ -3420,9 +3512,10 @@ def _assert_frag_2_at_ts_2(resp):
             resp = req.get_response(self.object_controller)
             if policy.policy_type == EC_POLICY:
                 _assert_frag_2_at_ts_2(resp)
+                self.assertECBodyEqual(resp, 'NEWER')
             else:
                 _assert_repl_data_at_ts_2()
-            self.assertEqual(resp.body, 'NEWER')
+                self.assertEqual(resp.body, 'NEWER')
 
             req = Request.blank('/sda1/p/a/c/o', headers=headers,
                                 environ={'REQUEST_METHOD': 'HEAD'})
@@ -6748,7 +6841,7 @@ def _check_multiphase_put_commit_handling(self,
         the context at the commit phase (after getting the second expect-100
         continue response.
 
-        It can setup a resonable stub request, but you can over-ride some
+        It can setup a reasonable stub request, but you can over-ride some
         characteristics of the request via kwargs.
 
         :param test_doc: first part of the mime conversation before the object
@@ -6756,11 +6849,11 @@ def _check_multiphase_put_commit_handling(self,
                          object body
         :param headers: headers to send along with the initial request; some
                         object-metadata (e.g.  X-Backend-Obj-Content-Length)
-                        is generally expected tomatch the test_doc)
+                        is generally expected to match the test_doc)
         :param finish_body: boolean, if true send "0\r\n\r\n" after test_doc
                             and wait for 100-continue before yielding context
         """
-        test_data = 'obj data'
+        test_data = encode_frag_archive_bodies(POLICIES[1], 'obj data')[0]
         footer_meta = {
             "X-Object-Sysmeta-Ec-Frag-Index": "2",
             "Etag": md5(test_data).hexdigest(),
@@ -6782,7 +6875,6 @@ def _check_multiphase_put_commit_handling(self,
 
         # phase1 - PUT request with object metadata in footer and
         # multiphase commit conversation
-        put_timestamp = utils.Timestamp(time())
         headers = headers or {
             'Content-Type': 'text/plain',
             'Transfer-Encoding': 'chunked',
diff --git a/test/unit/obj/test_ssync.py b/test/unit/obj/test_ssync.py
index 10f7f3c..29b9902 100644
--- a/test/unit/obj/test_ssync.py
+++ b/test/unit/obj/test_ssync.py
@@ -26,13 +26,13 @@
 from swift.common.exceptions import DiskFileNotExist, DiskFileError, \
     DiskFileDeleted
 from swift.common import utils
-from swift.common.storage_policy import POLICIES
+from swift.common.storage_policy import POLICIES, EC_POLICY
 from swift.common.utils import Timestamp
 from swift.obj import ssync_sender, server
 from swift.obj.reconstructor import RebuildingECDiskFileStream, \
     ObjectReconstructor
 
-from test.unit import patch_policies, debug_logger
+from test.unit import patch_policies, debug_logger, encode_frag_archive_bodies
 from test.unit.obj.common import BaseTest, FakeReplicator
 
 
@@ -73,6 +73,7 @@ def setUp(self):
         self.rx_node = {'replication_ip': self.rx_ip,
                         'replication_port': self.rx_port,
                         'device': self.device}
+        self.obj_data = {}  # maps obj path -> obj data
 
     def tearDown(self):
         self.rx_server.kill()
@@ -116,27 +117,27 @@ def wrapped_connect():
             sender.readline = make_readline_wrapper(sender.readline)
         return wrapped_connect, trace
 
+    def _get_object_data(self, path, **kwargs):
+        # return data for given path
+        if path not in self.obj_data:
+            self.obj_data[path] = '%s___data' % path
+        return self.obj_data[path]
+
     def _create_ondisk_files(self, df_mgr, obj_name, policy, timestamp,
                              frag_indexes=None, commit=True):
-        frag_indexes = [None] if frag_indexes is None else frag_indexes
+        frag_indexes = frag_indexes or [None]
         metadata = {'Content-Type': 'plain/text'}
         diskfiles = []
         for frag_index in frag_indexes:
-            object_data = '/a/c/%s___%s' % (obj_name, frag_index)
-            if frag_index is not None:
+            object_data = self._get_object_data('/a/c/%s' % obj_name,
+                                                frag_index=frag_index)
+            if policy.policy_type == EC_POLICY:
                 metadata['X-Object-Sysmeta-Ec-Frag-Index'] = str(frag_index)
             df = self._make_diskfile(
                 device=self.device, partition=self.partition, account='a',
                 container='c', obj=obj_name, body=object_data,
                 extra_metadata=metadata, timestamp=timestamp, policy=policy,
                 frag_index=frag_index, df_mgr=df_mgr, commit=commit)
-            if commit:
-                df.open()
-                # sanity checks
-                listing = os.listdir(df._datadir)
-                self.assertTrue(listing)
-                for filename in listing:
-                    self.assertTrue(filename.startswith(timestamp.internal))
             diskfiles.append(df)
         return diskfiles
 
@@ -171,7 +172,8 @@ def _verify_diskfile_sync(self, tx_df, rx_df, frag_index, same_etag=False):
             else:
                 self.assertEqual(v, rx_metadata.pop(k), k)
         self.assertFalse(rx_metadata)
-        expected_body = '%s___%s' % (tx_df._name, frag_index)
+        expected_body = self._get_object_data(tx_df._name,
+                                              frag_index=frag_index)
         actual_body = ''.join([chunk for chunk in rx_df.reader()])
         self.assertEqual(expected_body, actual_body)
 
@@ -304,7 +306,23 @@ def _verify_tombstones(self, tx_objs, policy):
 
 
 @patch_policies(with_ec_default=True)
-class TestSsyncEC(TestBaseSsync):
+class TestBaseSsyncEC(TestBaseSsync):
+    def setUp(self):
+        super(TestBaseSsyncEC, self).setUp()
+        self.policy = POLICIES.default
+
+    def _get_object_data(self, path, frag_index=None, **kwargs):
+        # return a frag archive for given object name and frag index.
+        # for EC policies obj_data maps obj path -> list of frag archives
+        if path not in self.obj_data:
+            # make unique frag archives for each object name
+            data = path * 2 * (self.policy.ec_ndata + self.policy.ec_nparity)
+            self.obj_data[path] = encode_frag_archive_bodies(
+                self.policy, data)
+        return self.obj_data[path][frag_index]
+
+
+class TestSsyncEC(TestBaseSsyncEC):
     def test_handoff_fragment_revert(self):
         # test that a sync_revert type job does send the correct frag archives
         # to the receiver
@@ -334,7 +352,7 @@ def test_handoff_fragment_revert(self):
         tx_objs['o3'] = self._create_ondisk_files(
             tx_df_mgr, 'o3', policy, t3, (rx_node_index,))
         rx_objs['o3'] = self._create_ondisk_files(
-            rx_df_mgr, 'o3', policy, t3, (14,))
+            rx_df_mgr, 'o3', policy, t3, (13,))
         # o4 primary and handoff fragment archives on tx, handoff in sync on rx
         t4 = next(self.ts_iter)
         tx_objs['o4'] = self._create_ondisk_files(
@@ -380,7 +398,8 @@ def test_handoff_fragment_revert(self):
                 self.assertTrue(
                     'X-Object-Sysmeta-Ec-Frag-Index: %s' % rx_node_index
                     in subreq.get('headers'))
-                expected_body = '%s___%s' % (subreq['path'], rx_node_index)
+                expected_body = self._get_object_data(subreq['path'],
+                                                      rx_node_index)
                 self.assertEqual(expected_body, subreq['body'])
             elif subreq.get('method') == 'DELETE':
                 self.assertEqual('/a/c/o5', subreq['path'])
@@ -427,7 +446,7 @@ def test_handoff_fragment_only_missing_durable(self):
         tx_objs[obj_name] = self._create_ondisk_files(
             tx_df_mgr, obj_name, policy, t2, (tx_node_index, rx_node_index,))
         rx_objs[obj_name] = self._create_ondisk_files(
-            rx_df_mgr, obj_name, policy, t2, (13, 14), commit=False)
+            rx_df_mgr, obj_name, policy, t2, (12, 13), commit=False)
         expected_subreqs['PUT'].append(obj_name)
 
         # o3 on rx has frag at other time missing .durable - PUT required
@@ -486,7 +505,8 @@ def test_handoff_fragment_only_missing_durable(self):
                             % (method, obj, expected_subreqs[method]))
             expected_subreqs[method].remove(obj)
             if method == 'PUT':
-                expected_body = '%s___%s' % (subreq['path'], rx_node_index)
+                expected_body = self._get_object_data(
+                    subreq['path'], frag_index=rx_node_index)
                 self.assertEqual(expected_body, subreq['body'])
         # verify all expected subreqs consumed
         for _method, expected in expected_subreqs.items():
@@ -549,7 +569,8 @@ def fake_reconstruct_fa(job, node, metadata):
             if len(reconstruct_fa_calls) == 2:
                 # simulate second reconstruct failing
                 raise DiskFileError
-            content = '%s___%s' % (metadata['name'], rx_node_index)
+            content = self._get_object_data(metadata['name'],
+                                            frag_index=rx_node_index)
             return RebuildingECDiskFileStream(
                 metadata, rx_node_index, iter([content]))
 
@@ -584,7 +605,8 @@ def fake_reconstruct_fa(job, node, metadata):
                 self.assertTrue(
                     'X-Object-Sysmeta-Ec-Frag-Index: %s' % rx_node_index
                     in subreq.get('headers'))
-                expected_body = '%s___%s' % (subreq['path'], rx_node_index)
+                expected_body = self._get_object_data(
+                    subreq['path'], frag_index=rx_node_index)
                 self.assertEqual(expected_body, subreq['body'])
             elif subreq.get('method') == 'DELETE':
                 self.assertEqual('/a/c/o5', subreq['path'])
@@ -660,13 +682,22 @@ def test_send_invalid_frag_index(self):
 
 
 class FakeResponse(object):
-    def __init__(self, frag_index, data):
+    def __init__(self, frag_index, obj_data, length=None):
         self.headers = {
             'X-Object-Sysmeta-Ec-Frag-Index': str(frag_index),
             'X-Object-Sysmeta-Ec-Etag': 'the etag',
             'X-Backend-Timestamp': '1234567890.12345'
         }
-        self.data = data
+        self.frag_index = frag_index
+        self.obj_data = obj_data
+        self.data = ''
+        self.length = length
+
+    def init(self, path):
+        if isinstance(self.obj_data, Exception):
+            self.data = self.obj_data
+        else:
+            self.data = self.obj_data[path][self.frag_index]
 
     def getheaders(self):
         return self.headers
@@ -676,14 +707,12 @@ def read(self, length):
             raise self.data
         val = self.data
         self.data = ''
-        return val
+        return val if self.length is None else val[:self.length]
 
 
-@patch_policies(with_ec_default=True)
-class TestSsyncECReconstructorSyncJob(TestBaseSsync):
+class TestSsyncECReconstructorSyncJob(TestBaseSsyncEC):
     def setUp(self):
         super(TestSsyncECReconstructorSyncJob, self).setUp()
-        self.policy = POLICIES.default
         self.rx_node_index = 0
         self.tx_node_index = 1
 
@@ -731,6 +760,11 @@ def fake_get_response(recon, node, part, path, headers, policy):
             if path not in path_to_responses:
                 path_to_responses[path] = frag_responses.pop(0)
             response = path_to_responses[path].pop()
+            # the frag_responses list is in ssync task order, we only know the
+            # path when consuming the responses so initialise the path in the
+            # response now
+            if response:
+                response.init(path)
             fake_get_response_calls.append(path)
             return response
 
@@ -742,42 +776,34 @@ def fake_get_part_nodes(part):
             return (self.policy.object_ring._get_part_nodes(part) +
                     [self.job_node])
 
-        def fake_reconstruct(self, policy, fragment_payload, frag_index):
-            # fake EC reconstruction by returning first frag, which is ok
-            # because all frags would be same length
-            return fragment_payload[0]
-
         with mock.patch(
                 'swift.obj.reconstructor.ObjectReconstructor._get_response',
-                fake_get_response):
-            with mock.patch(
-                    'swift.obj.reconstructor.ObjectReconstructor._reconstruct',
-                    fake_reconstruct):
-                with mock.patch.object(
-                        self.policy.object_ring, 'get_part_nodes',
-                        fake_get_part_nodes):
-                    self.reconstructor = ObjectReconstructor(
-                        {}, logger=debug_logger('test_reconstructor'))
-                    job = {
-                        'device': self.device,
-                        'partition': self.partition,
-                        'policy': self.policy,
-                        'sync_diskfile_builder':
-                            self.reconstructor.reconstruct_fa
-                    }
-                    sender = ssync_sender.Sender(
-                        self.daemon, self.job_node, job, self.suffixes)
-                    sender.connect, trace = self.make_connect_wrapper(sender)
-                    sender()
+                fake_get_response), \
+                mock.patch.object(
+                    self.policy.object_ring, 'get_part_nodes',
+                    fake_get_part_nodes):
+            self.reconstructor = ObjectReconstructor(
+                {}, logger=debug_logger('test_reconstructor'))
+            job = {
+                'device': self.device,
+                'partition': self.partition,
+                'policy': self.policy,
+                'sync_diskfile_builder':
+                    self.reconstructor.reconstruct_fa
+            }
+            sender = ssync_sender.Sender(
+                self.daemon, self.job_node, job, self.suffixes)
+            sender.connect, trace = self.make_connect_wrapper(sender)
+            sender()
         return trace
 
     def test_sync_reconstructor_partial_rebuild(self):
         # First fragment to sync gets partial content from reconstructor.
         # Expect ssync job to exit early with no file written on receiver.
         frag_responses = [
-            [FakeResponse(i, 'x' * (self.frag_length - 1))
+            [FakeResponse(i, self.obj_data, length=-1)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)],
-            [FakeResponse(i, 'y' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)]]
 
         self._test_reconstructor_sync_job(frag_responses)
@@ -809,9 +835,9 @@ def test_sync_reconstructor_no_rebuilt_content(self):
         # reconstructor. Expect ssync job to exit early with no file written on
         # receiver.
         frag_responses = [
-            [FakeResponse(i, '')
+            [FakeResponse(i, self.obj_data, length=0)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)],
-            [FakeResponse(i, 'y' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)]]
 
         self._test_reconstructor_sync_job(frag_responses)
@@ -845,7 +871,7 @@ def test_sync_reconstructor_exception_during_rebuild(self):
         frag_responses = [
             # ec_ndata responses are ok, but one of these will be ignored as
             # it is for the frag index being rebuilt
-            [FakeResponse(i, 'x' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata)] +
             # ec_nparity responses will raise an Exception - at least one of
             # these will be used during rebuild
@@ -853,7 +879,7 @@ def test_sync_reconstructor_exception_during_rebuild(self):
              for i in range(self.policy.ec_ndata,
                             self.policy.ec_ndata + self.policy.ec_nparity)],
             # second set of response are all good
-            [FakeResponse(i, 'y' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)]]
 
         self._test_reconstructor_sync_job(frag_responses)
@@ -890,7 +916,7 @@ def test_sync_reconstructor_no_responses(self):
         frag_responses = [
             [None
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)],
-            [FakeResponse(i, 'y' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)]]
 
         trace = self._test_reconstructor_sync_job(frag_responses)
@@ -900,15 +926,18 @@ def test_sync_reconstructor_no_responses(self):
         self.assertEqual(1, len(results['tx_updates']))
         self.assertFalse(results['rx_updates'])
         self.assertEqual('PUT', results['tx_updates'][0].get('method'))
-        synced_obj_name = results['tx_updates'][0].get('path')[-2:]
+        synced_obj_path = results['tx_updates'][0].get('path')
+        synced_obj_name = synced_obj_path[-2:]
 
         msgs = []
         obj_name = synced_obj_name
         try:
             df = self._open_rx_diskfile(
                 obj_name, self.policy, self.rx_node_index)
-            self.assertEqual('y' * self.frag_length,
-                             ''.join([d for d in df.reader()]))
+            self.assertEqual(
+                self._get_object_data(synced_obj_path,
+                                      frag_index=self.rx_node_index),
+                ''.join([d for d in df.reader()]))
         except DiskFileNotExist:
             msgs.append('Missing rx diskfile for %r' % obj_name)
 
@@ -936,9 +965,9 @@ def test_sync_reconstructor_rebuild_ok(self):
         # Sanity test for this class of tests. Both fragments get a full
         # complement of responses and rebuild correctly.
         frag_responses = [
-            [FakeResponse(i, 'x' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)],
-            [FakeResponse(i, 'y' * self.frag_length)
+            [FakeResponse(i, self.obj_data)
              for i in range(self.policy.ec_ndata + self.policy.ec_nparity)]]
 
         trace = self._test_reconstructor_sync_job(frag_responses)
@@ -948,19 +977,18 @@ def test_sync_reconstructor_rebuild_ok(self):
         self.assertEqual(2, len(results['tx_updates']))
         self.assertFalse(results['rx_updates'])
         msgs = []
-        rx_frags = []
         for obj_name in self.tx_objs:
             try:
                 df = self._open_rx_diskfile(
                     obj_name, self.policy, self.rx_node_index)
-                rx_frags.append(''.join([d for d in df.reader()]))
+                self.assertEqual(
+                    self._get_object_data(df._name,
+                                          frag_index=self.rx_node_index),
+                    ''.join([d for d in df.reader()]))
             except DiskFileNotExist:
                 msgs.append('Missing rx diskfile for %r' % obj_name)
         if msgs:
             self.fail('Failed with:\n%s' % '\n'.join(msgs))
-        self.assertIn('x' * self.frag_length, rx_frags)
-        self.assertIn('y' * self.frag_length, rx_frags)
-        self.assertFalse(rx_frags[2:])
         self.assertFalse(self.daemon.logger.get_lines_for_level('error'))
         self.assertFalse(
             self.reconstructor.logger.get_lines_for_level('error'))
@@ -1050,7 +1078,7 @@ def test_sync(self):
             if subreq.get('method') == 'PUT':
                 self.assertTrue(
                     subreq['path'] in ('/a/c/o1', '/a/c/o2', '/a/c/o3'))
-                expected_body = '%s___None' % subreq['path']
+                expected_body = self._get_object_data(subreq['path'])
                 self.assertEqual(expected_body, subreq['body'])
             elif subreq.get('method') == 'DELETE':
                 self.assertTrue(subreq['path'] in ('/a/c/o5', '/a/c/o7'))
@@ -1224,7 +1252,7 @@ def test_meta_file_sync(self):
                             % (method, obj, expected_subreqs[method]))
             expected_subreqs[method].remove(obj)
             if method == 'PUT':
-                expected_body = '%s___None' % subreq['path']
+                expected_body = self._get_object_data(subreq['path'])
                 self.assertEqual(expected_body, subreq['body'])
         # verify all expected subreqs consumed
         for _method, expected in expected_subreqs.items():
@@ -1463,7 +1491,7 @@ def test_content_type_sync(self):
                             % (method, obj, expected_subreqs[method]))
             expected_subreqs[method].remove(obj)
             if method == 'PUT':
-                expected_body = '%s___None' % subreq['path']
+                expected_body = self._get_object_data(subreq['path'])
                 self.assertEqual(expected_body, subreq['body'])
         # verify all expected subreqs consumed
         for _method, expected in expected_subreqs.items():
diff --git a/test/unit/proxy/controllers/test_obj.py b/test/unit/proxy/controllers/test_obj.py
index ed97eb9..4ceccb6 100755
--- a/test/unit/proxy/controllers/test_obj.py
+++ b/test/unit/proxy/controllers/test_obj.py
@@ -43,7 +43,8 @@
 from swift.common.storage_policy import POLICIES, ECDriverError, StoragePolicy
 
 from test.unit import FakeRing, FakeMemcache, fake_http_connect, \
-    debug_logger, patch_policies, SlowBody, FakeStatus
+    debug_logger, patch_policies, SlowBody, FakeStatus, \
+    encode_frag_archive_bodies
 from test.unit.proxy.test_server import node_error_count
 
 
@@ -2205,22 +2206,7 @@ def test_PUT_old_obj_server(self):
 
     def _make_ec_archive_bodies(self, test_body, policy=None):
         policy = policy or self.policy
-        segment_size = policy.ec_segment_size
-        # split up the body into buffers
-        chunks = [test_body[x:x + segment_size]
-                  for x in range(0, len(test_body), segment_size)]
-        # encode the buffers into fragment payloads
-        fragment_payloads = []
-        for chunk in chunks:
-            fragments = self.policy.pyeclib_driver.encode(chunk)
-            if not fragments:
-                break
-            fragment_payloads.append(fragments)
-
-        # join up the fragment payloads per node
-        ec_archive_bodies = [''.join(frags)
-                             for frags in zip(*fragment_payloads)]
-        return ec_archive_bodies
+        return encode_frag_archive_bodies(policy, test_body)
 
     def _make_ec_object_stub(self, test_body=None, policy=None,
                              timestamp=None):
